% !TEX root = optapprox.tex

%
%==========================================
\section{Accelerated gradient method}\label{sec:}
%==========================================
%
%
%---------------------------------------
\begin{yellow}
\begin{algorithm}[H]
\caption{Adaptive \textbf{AGM}} 
\label{algorithm:adaptiveGM} 
%
Inputs: $X_0$, $x_0\in X_0$, $t_0>0$, $\lambda>0$, $\beta>0$. Set $y_0=x_0$ and $k=0$.
%
\begin{itemize}
\item[(1)] While $Q^*_{t_k}(y_k, X_{k})< f(\widetilde{x}(y_k,t_k, X_{k}))$:\quad $t_k = t_k/2$.
\item[(2)] $x_{k+1} = \widetilde{x}(y_k,t_k,X_{k})$.
\item[(3)] $y_{k+1} = x_{k+1} + \beta(x_{k+1}-x_k)$.
\item[(4)] If $\eta^2(x_{k+1}, X_{k})> \qred\eta^2(x_{k}, X_{k}) + \lambda t_{k}(f(x_{k})-f(x_{k+1}))$:\quad $X_{k}=\REF(X_{k}, \eta(x_k,X_k))$\\
\item[(5)] $t_k = 2t_k$.
\item[(6)] Increment $k$ and go to (1).
\end{itemize}
%
\end{algorithm}
\end{yellow}
%---------------------------------------
%
%---------------------------------------
\begin{lemma}\label{lemma:}
The iterates of AGM satisfy
%
\begin{equation}\label{eq:agm:goodestimate1}
f(x_{k+1})  \le f(y_k) -\frac{t_k}{2 }\norm{P_k \nabla f(y_k)}^2 
\end{equation}
%
%
\begin{equation}\label{eq:agm:goodestimate2}
f(x_{k+1}) -f(x_k) \le \scp{P_k\nabla f(y_k)}{y_k-x_k} - \frac{t_k}{2}\norm{P_{k}\nabla f(y_k)}^2- \frac{1}{2L}\norm{P_k\nabla f(y_k)-P_k\nabla f(x_k)}^2
\end{equation}
%
and
%
\begin{equation}\label{eq:agm:goodestimate3}
f(x_{k+1}) -f(x^*) \le \scp{P_k\nabla f(y_k)}{y_k-x^*}- \frac{t_k}{2}\norm{P_{k}\nabla f(y_k)}^2
+ \frac{\Crel^2}{2\mu}\eta^2(y_k,X_k)).
\end{equation}
%
\end{lemma}
%
%---------------------------------------
\begin{proof}
(\ref{eq:agm:goodestimate1}) follows from the line search:
%
\begin{align*}
f(x_{k+1}) \le Q_{t_k}(y_k,x_{k+1}) = Q^*_{t_k}(y_k) = f(y_k)- \frac{t_k}{2}\norm{P_{k}\nabla f(y_k)}^2.
\end{align*}
%
Next we have by convexity and Lipschitz continuity
%
\begin{align*}
f(x_k) \ge f(y_k) + \scp{P_k\nabla f(y_k)}{x_k-y_k} + \frac{1}{2L}\norm{P_k\nabla f(y_k)-P_k\nabla f(x_k)}^2,
\end{align*}
%
which, subtracted from (\ref{eq:agm:goodestimate1}) gives (\ref{eq:agm:goodestimate2}).

Similarly, we have
%
\begin{align*}
f(x^*) \ge f(y_k) + \scp{\nabla f(y_k)}{x^*-y_k} + \frac{\mu}{2}\norm{x^*-y_k}^2,
\end{align*}
%
which gives with (\ref{eq:agm:goodestimate1})
%
\begin{align*}
f(x_{k+1}) - f(x^*) \le& \scp{P_k\nabla f(y_k)}{y_k-x^*}- \frac{t_k}{2}\norm{P_{k}\nabla f(y_k)}^2
+ \scp{(I-P_k)\nabla f(y_k)}{y_k-x^*}-\frac{\mu}{2}\norm{x^*-y_k}^2\\
\le&\scp{P_k\nabla f(y_k)}{y_k-x^*}- \frac{t_k}{2}\norm{P_{k}\nabla f(y_k)}^2
+ \frac{1}{2\mu}\norm{(I-P_k)\nabla f(y_k)}^2\\
\le&\scp{P_k\nabla f(y_k)}{y_k-x^*}- \frac{t_k}{2}\norm{P_{k}\nabla f(y_k)}^2
+ \frac{\Crel^2}{2\mu}\eta^2(y_k,X_k)
\end{align*}
%
\end{proof}
%
%---------------------------------------
\begin{lemma}\label{lemma:}
Let 
%
\begin{equation}\label{eq:}
\Delta f_k := f(x_k)-f(x^*) \quad\mbox{and}\quad \overline{x}_{k+1} =  x_{k+1}+\frac{\beta}{1-\beta} (x_{k+1}-x_k).
\end{equation}
%
We have
%
\begin{equation}\label{eq:}
\Delta f_{k+1} - \beta\Delta f_{k} \le \frac{1-\beta}{2t_k} \left( \norm{\overline{x}_{k}-x^*}^2 - \norm{\overline{x}_{k+1}-x^*}^2\right) + \frac{(1-\beta)\Crel^2}{2\mu}\eta^2(y_k,X_k)
\end{equation}
%
\end{lemma}
%
%---------------------------------------
\begin{proof}
Multiplying (\ref{eq:agm:goodestimate2}) by $\beta$ and (\ref{eq:agm:goodestimate3}) by $1-\beta$ we have
%
\begin{align*}
\Delta f_{k+1} - \beta \Delta f_{k} \le \scp{P_k\nabla f(y_k)}{y_k-\beta x_k - (1-\beta) x^*}- \frac{t_k}{2}\norm{P_{k}\nabla f(y_k)}^2
+ (1-\beta)\frac{\Crel^2}{2\mu}\eta^2(y_k,X_k)
\end{align*}
%
By the binomial formula $2ab - a^2 = b^2-(b-a)^2$ and the update rule for $y_k$ we have
%
\begin{align*}
2\scp{t_kP_k\nabla f(y_k)}{y_k-\beta x_k - (1-\beta) x^*}- \norm{t_k P_{k}\nabla f(y_k)}^2\\
=
\norm{y_k-\beta x_k - (1-\beta) x^*}^2 - \norm{y_k-\beta x_k - (1-\beta) x^* - t_k P_{k}\nabla f(y_k)}^2\\
=\norm{x_k-\beta x_{k-1} - (1-\beta) x^*}^2 - \norm{x_{k+1}-\beta x_k - (1-\beta) x^* }^2\\
=(1-\beta)\left( \norm{\overline{x}_{k}-x^*}^2 - \norm{\overline{x}_{k+1}-x^*}^2\right)
\end{align*}
%
%
\end{proof}
%
%
%---------------------------------------
\begin{theorem}\label{thm:}
We suppose that $f$ is continuously differentiable, $\mu$-strongly convex,
the level set $\mathcal L_f(x_0)$ is bounded and $\nabla f$ is $L$-Lipschitz on $\mathcal L_f(x_0)$.

Suppose that
%
\begin{equation}\label{eq:timestepcond}
\overline{t}\ge t_k \ge \underline{t}>0\quad\forall k\in\N.
\end{equation}
%
Let
%
\begin{equation}\label{eq:gm:errordef}
e_k := f(x_{k}) - f(x^*) + C_1 \eta^2(x_{k}, X_k),\qquad C_1 := \frac{\Crel^2}{\mu}.
\end{equation}
%
Then we have for all $m,n\in\N$ and arbitrary $\lambda>0$
%
\begin{equation}\label{eq:gm:rconv}
e_{m+n} \le (C+1) \rho^n e_m,\quad C=\max\Set{\frac{1}{4\mu\underline{t}} +  \frac{2\max\Set{\lambda,2\Cstab^2}\Crel^2}{\mu(1-{\qred})} \overline{t},\,\frac{1+\qred}{1-\qred}},
\quad \rho = 1 - 1/(C+1).
\end{equation}
%
\end{theorem}
%
%---------------------------------------
\begin{remark}\label{rmk:}
Supposing that $\underline{t}$ and $\overline{t}$ are proportional to $L$, we find that $C$ is proportional to $\kappa_f = L/\mu$ as in the standard gradient method.
\end{remark}
%
%---------------------------------------
\begin{proof}
We first claim that
%
\begin{equation}\label{eq:gm:help2}
\eta^2(x_{k+1},X_{k+1}) \le \qred \eta^2(x_{k}, X_k) + \widetilde{\lambda} \overline{t} \left(f(x_{k})-f(x_{k+1})\right),\quad\widetilde{\lambda} := \max\Set{\lambda,2\Cstab^2}. 
\end{equation}
%
If no refinement happens, this follows by rule (4) of the algorithm and the assumption (\ref{eq:timestepcond}).
If a refinement step happens from $k$ to $k+1$, we have by (\ref{hyp:estimator:reduction}) and (\ref{eq:gm:goodestimate1})
%
\begin{align*}
\eta^2(x_{k+1},X_{k+1}) \le& \qred \eta^2(x_{k}, X_k) + \Cstab^2\norm{x_{k+1}-x_{k}}^2\\
\le& \qred \eta^2(x_{k}, X_k) + 2\Cstab^2 t_k \left(f(x_{k})-f(x_{k+1})\right).
\end{align*}
%

Now let
%
\begin{align*}
\Delta_{k} := f(x_{k}) -f(x^*),\qquad \eta^2_k = \eta^2(x_{k}, X_k),\qquad \zeta_k:= \norm{x_k-x^*}^2.
\end{align*}
%

From  (\ref{eq:gm:goodestimate2}), (\ref{eq:gm:help2}) and the assumption on the step-length (\ref{eq:timestepcond}),  we have  for $\beta:=\frac{2\Crel^2}{\mu(1-\qred)}$ 
%
\begin{align*}
\Delta_{k+1} + \beta\eta^2_{k+1} \le  \left(\qred + \frac{\Crel^2}{\mu \beta}\right) \beta\eta^2_{k} 
+ \widetilde{\lambda}\beta \overline{t} \left(f(x_{k})-f(x_{k+1})\right) + \frac{1}{2\underline{t}}\left(\zeta_k - \zeta_{k+1}\right).
\end{align*}
%
such that with $\widetilde{\qred}:= \qred + \frac{\Crel^2}{\mu \beta} = \frac12(1+\qred)<1$
%
\begin{align*}
\Delta_{k+1} + \beta\eta^2_{k+1} \le
\widetilde{\qred} \beta\eta^2_{k}
+ \widetilde{\lambda}\beta \overline{t} \left(f(x_{k})-f(x_{k+1})\right)
+ \frac{1}{\underline{2t}}\left(\zeta_k - \zeta_{k+1}\right)
\end{align*}
%
Summing up  yields
%
\begin{align*}
\sum_{k=n+1}^{N+1}\left(\Delta_{k} + \beta\eta^2_{k}\right)
\le& 
\widetilde{\qred}\beta\sum_{k=n}^{N}\eta^2_k
+ \widetilde{\lambda}\beta \overline{t} \left(f(x_{n})-f(x_{N+1})\right)
+ \frac{1}{2\underline{t}}\left(\zeta_n - \zeta_{N+1}\right) 
\end{align*}
%
such that
%
\begin{align*}
\sum_{k=n+1}^{N+1}\Delta_{k} + (1-\widetilde{\qred})\beta\sum_{k=n+1}^{N+1}\eta^2_k
\le 
\widetilde{\qred}\beta\eta^2_n
+  \widetilde{\lambda}\beta \overline{t} \left(f(x_{n})-f(x_{N+1})\right)
+ \frac{1}{2\underline{t}}\left(\zeta_n - \zeta_{N+1}\right) 
\end{align*}
%
This proves $\lim\limits_{N\to\infty}x_N \to x^*$ and then, with $\zeta_n=\norm{x_n-x^*}^2 \le \frac{2}{\mu} \Delta_n$,
%
\begin{align*}
\sum_{k=n+1}^{\infty}\Delta_{k} + (1-\widetilde{\qred})\beta\sum_{k=n+1}^{\infty}\eta^2_k
\le &
\widetilde{\qred}\beta\eta^2_n
+  \widetilde{\lambda}\beta \overline{t} \left(f(x_{n})-f(x^*)\right)
+ \frac{1}{2\underline{t}}\zeta_n\\
\le & 
\left( \widetilde{\lambda}\beta \overline{t}+ \frac{1}{4\mu\underline{t}}\right) \Delta_{n} + \widetilde{\qred}\beta\eta^2_n
\end{align*}
%
With
%
\begin{align*}
C_1 = (1-\widetilde{\qred})\beta = \frac{\Crel^2}{\mu}
\end{align*}
%
we have
%
\begin{align*}
\sum_{k=n+1}^{\infty}\left( \Delta_{k} + C_1\eta^2_k\right)
\le&
\left( \frac{1}{4\mu\underline{t}}+  \widetilde{\lambda}\beta \overline{t}\right) \Delta_{n} + \widetilde{\qred}\beta\eta^2_n\\
\le&\left( \frac{1}{4\mu\underline{t}}+  \frac{\widetilde{\lambda}C_1}{1-\widetilde{\qred}} \overline{t}\right) \Delta_{n} + \frac{\widetilde{\qred}}{1-\widetilde{\qred}}C_1\eta^2_n\\
=&\left( \frac{1}{4\mu\underline{t}}+  \frac{2\widetilde{\lambda}C_1}{1-{\qred}} \overline{t}\right) \Delta_{n} + \frac{1+{\qred}}{1-{\qred}}C_1\eta^2_n
\end{align*}
%
%
\end{proof}
%


%
%---------------------------------------
\begin{theorem}\label{thm:}
If $\lambda$ satisfies
%
\begin{equation}\label{eq:}
\lambda \ge 2\Cstab + 8\kappa_f^2\frac{1-\qred}{\underline{t}^2} 
\end{equation}
%
we have
%
\begin{align*}
\sum_{k=0}^n \dim X_k \le C \eps_n^{-1/s}\quad \forall n\in\N.
\end{align*}
%
\end{theorem}
%
%---------------------------------------
\begin{proof}
%
By the Lipschitz-continuity we have
%
\begin{align*}
\norm{(I-P_{X_k})\nabla f(x_k)} = \norm{(I-P_{X_k})(\nabla f(x_k)-\nabla f(x^*)}
\le L \norm{x^*-x_k},
\end{align*}
%
such that
%
\begin{align*}
\norm{(I-P_{X_k})\nabla f(x_k)}^2 \le \frac{L^2}{\mu} (f(x_k) - f(x^*)). 
\end{align*}
%
Let  $\widetilde{X}_k\in \mathcal X(X_0)$ and $\widetilde{x}_k:=\argmin_{x\in \widetilde{X}_k}f(x)$.
If $f(\widetilde{x}_k) - f(x^*)\le \gamma e_k$ we have
%
\begin{align*}
f(x_k) - f(x^*) =& f(x_k) - f(\widetilde{x}_k) + f(\widetilde{x}_k) - f(x^*)\\
\le& f(x_k) - f(\widetilde{x}_k) + \gamma(f(x_k) - f(x^*) + C_1\eta^2(x_k,X_k))
\end{align*}
%
and then for $\gamma <1$
%
\begin{align*}
(1-\gamma) f(x_k) - f(x^*) \le (f(x_k) - f(\widetilde{x}_k)) + \gamma C_1\eta^2(x_k,X_k)
\end{align*}
%


By strong convexity  we have 
%
\begin{align*}
f(x_k) - f(\widetilde{x}_k) \le& \scp{\nabla f(x_k)}{x_k-\widetilde{x}_k} - \frac{\mu}{2}\norm{x_k-\widetilde{x}_k}^2\\
=& \scp{P_{X_k}\nabla f(x_k)}{x_k-\widetilde{x}_k} + \scp{(P_{\widetilde{X}_k}-P_{X_k})\nabla f(x_k)}{x_k-\widetilde{x}_k}- \frac{\mu}{2}\norm{x_k-\widetilde{x}_k}^2\\
\le& \frac{1}{t_k}\norm{x_{k+1}-x_k}\norm{x_k-\widetilde{x}_k} + \norm{(P_{\widetilde{X}_k}-P_{X_k})\nabla f(x_k)}\norm{x_k-\widetilde{x}_k}- \frac{\mu}{2}\norm{x_k-\widetilde{x}_k}^2\\
\le&\frac{1}{\underline{t}^2\mu}\norm{x_{k+1}-x_k}^2 + \frac{1}{\mu} \norm{(P_{\widetilde{X}_k}-P_{X_k})\nabla f(x_k)}^2
\end{align*}
%
From the refinement criterion we have
%
\begin{align*}
\eta^2(x_{k+1}, X_{k})> \qred\eta^2(x_{k}, X_{k}) + \lambda t_{k}(f(x_{k})-f(x_{k+1}))\ge
\qred\eta^2(x_{k}, X_{k}) + \lambda \norm{x_{k+1}-x_k}^2
\end{align*}
%
With (\ref{hyp:estimator:stability}) and (\ref{eq:gm:goodestimate1}) we have
%
\begin{align*}
\lambda \norm{x_{k+1}-x_k}^2 \le& (1-\qred)\eta^2(x_{k}, X_{k}) + \Cstab^2\norm{x_{k+1}-x_k}^2
\end{align*}
%
such that with $\xi := \lambda -  2\Cstab^2 >0$
%
\begin{align*}
\norm{x_{k+1}-x_k}^2 \le \frac{1-\qred}{\xi}\eta^2(x_{k}, X_{k}).
\end{align*}
%
%
Combining these inequalities we get with (\ref{hyp:estimator:reliability})
%
\begin{align*}
\norm{(I-P_{X_k})\nabla f(x_k)}^2 \le& \frac{L^2}{\mu} (f(x_k) - f(x^*))
\le \frac{L^2 }{\mu(1-\gamma)} \left( (f(x_k) - f(\widetilde{x}_k)) + \gamma C_1\eta^2(x_k,X_k)\right)\\
\le&  \frac{L^2 }{\mu(1-\gamma)} \left( \left( \frac{1-\qred}{\underline{t}^2\mu\xi}+ \gamma C_1\right)\eta^2(x_{k}, X_{k}) + \frac{1}{\mu} \norm{(P_{\widetilde{X}_k}-P_{X_k})\nabla f(x_k)}^2 \right)\\
\le&  \frac{L^2 }{\mu(1-\gamma)} \left( \left( \frac{1-\qred}{\underline{t}^2\mu\xi}+ \gamma C_1\right)\Ceff^2\norm{(I-P_{X_k})\nabla f(x_k)}^2 + \frac{1}{\mu} \norm{(P_{\widetilde{X}_k}-P_{X_k})\nabla f(x_k)}^2 \right)
\end{align*}
%
Then for 
%
\begin{align*}
\gamma \le \min\Set{\frac12,\frac{\mu}{4L^2 C_1 \Ceff^2}},\quad
\xi \ge \frac{8L^2 }{\mu} \frac{1-\qred}{\underline{t}^2\mu} 
\end{align*}
%
we finally have
%
\begin{align*}
\norm{(I-P_{X_k})\nabla f(x_k)}^2 \le  4\kappa_f^2\norm{(P_{\widetilde{X}_k}-P_{X_k})\nabla f(x_k)}^2
\end{align*}
%


\end{proof}
%
